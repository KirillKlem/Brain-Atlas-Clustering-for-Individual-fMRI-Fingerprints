# -*- coding: utf-8 -*-
"""aidao_track1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WKFYvdSOKylYawGkMLmU2Ntxszc8ceIG

## Data overview:
You are provided with unlabeled data for which labeling is to be computed. There are 20 subjects of fMRI scan in total, for each subject there are 16 (2x2x2x2) representations corresponding to – two different brain atlas partitions (Brainnetome and Schaefer200), times two different smoothing strategies, times two segments of scan sequence, times two different sequences. Shape structure for the dataset translates aas follows `[20*16 objects, 10 timesteps from scan sequence, 246 number of features in larger atlas]`. Note that since two atlases with different number of partitions are used, some data arrays are padded with `np.nan`s, so that data shape is uniform.

*Public Test Data:*

- IHB dataset: 10 subject

*Private Test Data:*

- IHB dataset: 10 subjects

File with all $20 \times 16$ scan data representations is available via this repository lfs.

## Objective
The task aims to simulate a realistic research workflow:

-	Data Collection Constraints: Collecting fMRI data is challenging and costly, resulting in small proprietary datasets.
-	Dataset Variability: These open datasets inherently differ from proprietary data in aspects such as scanner type, geographic location of data collection, and average age of participants. Such aspect are simulated through choice of different brain states, time sequences, atlas-based aggregations.
The primary goal is to develop a model capable of identifying a person using fMRI as a "fingerprint" which is consistent across different perturbations and aspects of data.

## Performance Metric and Deliverables

*Evaluation Metric:* Adjusted Rand Score (rescaled to be between 0.0 for random prediction and worse and 1.0 for perfect labeling: $\text{ari}(y, \hat{y}) = \frac{\text{ri}(y, \hat{y}) - 0.85}{0.15}$)

*Required Deliverables:*
-	`<name>.csv` submission file that contains column named `prediction` which has same integer labels for objects corresponding to same class (subject's fMRI scan).

**Use standard scaling to account for different scan smoothing strategies, kmeans for computing cluster centers and subsequent distance-based ranking**
"""

import numpy as np
import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import SpectralClustering
from sklearn.neighbors import kneighbors_graph

data = np.load('/content/drive/MyDrive/ihb.npy') #Сюда путь к данным

n_samples, n_timepoints, n_features = data.shape
df_data = pd.DataFrame(data.reshape(n_samples * n_timepoints, n_features))

# Иммутация пропущенных значений с использованием KNN
imputer = KNNImputer(n_neighbors=5)
df_imputed = pd.DataFrame(imputer.fit_transform(df_data))

# Возвращаем обратно в трехмерный массив для дальнейшей работы (разбиваем на временные точки)
df_imputed = df_imputed.values.reshape(n_samples, n_timepoints, n_features)

# Функция для преобразования Фишера Z
def fisher_z_transform(corr_matrix):
    z_corr_matrix = np.arctanh(corr_matrix)
    z_corr_matrix[np.isinf(z_corr_matrix)] = 0
    return z_corr_matrix

# Вычисление функциональной связности с применением DataFrame
fc_list = []
for i in range(len(df_imputed)):
    sample = df_imputed[i]  # Извлекаем один сэмпл
    corr_matrix = np.corrcoef(sample.T)
    z_corr_matrix = fisher_z_transform(corr_matrix)
    iu = np.triu_indices_from(z_corr_matrix, k=1)
    fc_vector = z_corr_matrix[iu]
    fc_list.append(fc_vector)

# Преобразуем в DataFrame для дальнейшей обработки
df_functional_connectivity = pd.DataFrame(fc_list)

# Заменяем NaN на средние значения
df_functional_connectivity.fillna(df_functional_connectivity.mean(), inplace=True)

scaler = StandardScaler()
df_fc_scaled = pd.DataFrame(scaler.fit_transform(df_functional_connectivity))

# Снижение размерности с помощью PCA
pca = PCA(n_components=50, random_state=42)
df_data_reduced = pd.DataFrame(pca.fit_transform(df_fc_scaled))

# Кластеризация с использованием kneighbors_graph
n_neighbors = 9
knn_graph = kneighbors_graph(df_data_reduced, n_neighbors=n_neighbors, include_self=False)
spectral_clustering = SpectralClustering(n_clusters=20, affinity='precomputed', random_state=42)
labels = spectral_clustering.fit_predict(knn_graph)

# Преобразуем метки в DataFrame для отправки
df_submission = pd.DataFrame({'prediction': labels})
df_submission.to_csv('submission_spectral.csv', index=False)